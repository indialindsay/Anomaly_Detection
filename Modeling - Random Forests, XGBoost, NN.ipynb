{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('train.csv')\n",
    "te = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33173, 17)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Timestamp', 'Values', 'Temperature', 'holiday',\n",
       "       'Weekday', 'Hour', 'Month', 'Day', 'Time Delta', 'value delta',\n",
       "       'tv delta', 'prev value', 'twice prev value', 'day shift',\n",
       "       'month shift', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial random forest fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting X and Y variables\n",
    "y_train = tr['tv delta']\n",
    "X_train = tr[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]\n",
    "\n",
    "y_test = te['tv delta']\n",
    "X_test = te[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 5.716350014724802\n",
      "RMSE for test set: 15.887416299573475\n",
      "Mean Absolute Error for train: 3.949370992318209\n",
      "Mean Absolute Error for test: 11.002088145230802\n",
      "Variable: prev value           Importance: 0.9\n",
      "Variable: Temperature          Importance: 0.03\n",
      "Variable: day shift            Importance: 0.03\n",
      "Variable: twice prev value     Importance: 0.02\n",
      "Variable: month shift          Importance: 0.02\n",
      "Variable: holiday              Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#scaling the data\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# create regressor object \n",
    "regressor = RandomForestRegressor(n_estimators = 100, random_state = 0) \n",
    "  \n",
    "# fit the regressor with x and y data \n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "tr_pred = regressor.predict(X_train) \n",
    "te_pred = regressor.predict(X_test) \n",
    "\n",
    "print(\"RMSE for training set:\", sqrt(mean_squared_error(y_train, tr_pred)))\n",
    "print(\"RMSE for test set:\", sqrt(mean_squared_error(y_test, te_pred)))\n",
    "\n",
    "print('Mean Absolute Error for train:', metrics.mean_absolute_error(y_train, tr_pred))\n",
    "print('Mean Absolute Error for test:', metrics.mean_absolute_error(y_test, te_pred))\n",
    "\n",
    "feature_list = ['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']\n",
    "# Get numerical feature importances\n",
    "importances = list(regressor.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are clearly overfitting. Let's finetune some parameters\n",
    "\n",
    "- n_estimators: number of trees built before taking avg of predictions. In general, a higher number of trees increases the performance and makes the predictions more stable, but it also slows down the computation\n",
    "\n",
    "- min_sample_leaf: The minimum number of samples required to be at a leaf node. A smaller leaf makes the model more prone to capturing noise in train data. We are overfitting so let's try increasing this.\n",
    "\n",
    "source: https://builtin.com/data-science/random-forest-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning(X_train,y_train,X_test,y_test, estimators, leaf):\n",
    "    ''' This function takes a list of estimator and leaf parameters and returns \n",
    "    the parameters that produced the lowest error'''\n",
    "    \n",
    "    arr = np.zeros((len(estimators),len(leaf))) #empty array to store scores\n",
    "    \n",
    "    #scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    for i in range(len(estimators)):\n",
    "        for j in range(len(leaf)):\n",
    "            est = estimators[i]\n",
    "            l = leaf[j]\n",
    "           \n",
    "             # create regressor object \n",
    "            regressor = RandomForestRegressor(n_estimators = est, min_samples_leaf=l, random_state = 0) \n",
    "  \n",
    "            # fit the regressor with x and y data \n",
    "            regressor.fit(X_train, y_train)\n",
    "\n",
    "            tr_pred = regressor.predict(X_train) \n",
    "            te_pred = regressor.predict(X_test) \n",
    "\n",
    "            test_score = sqrt(mean_squared_error(y_test, te_pred))\n",
    "            arr[i][j] = test_score\n",
    "    \n",
    "   \n",
    "    max_ = np.amin(arr) \n",
    "    inputs = np.where(arr == max_)\n",
    "    print(\"Lowest test RMSE score:\",max_)\n",
    "    best_est = estimators[inputs[0][0]]\n",
    "    best_leaf = leaf[inputs[1][0]]\n",
    "    print(\"Best # estimators:\",best_est)\n",
    "    print(\"Best # min samples leaf:\",best_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest test RMSE score: 15.140363398825533\n",
      "Best # estimators: 300\n",
      "Best # min samples leaf: 50\n"
     ]
    }
   ],
   "source": [
    "est = [10,100,200,300]\n",
    "leaf = [15,30,50,70]\n",
    "max_feats = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "arr = tuning(X_train,y_train,X_test,y_test,est,leaf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test RMSE dropped from the previous value of 15.88 to 15.14\n",
    "\n",
    "Let's see if increasing both the number of estimators and min samples leaf value will help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest test RMSE score: 15.140363398825533\n",
      "Best # estimators: 300\n",
      "Best # min samples leaf: 50\n"
     ]
    }
   ],
   "source": [
    "est = [300,500,700]\n",
    "leaf = [50,60,70]\n",
    "\n",
    "arr = tuning(X_train,y_train,X_test,y_test,est,leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300 and 50 is our best bet. Let's see whether 200 will perform as well as 300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 14.570057072199504\n",
      "RMSE for test set: 15.143103582113675\n"
     ]
    }
   ],
   "source": [
    "regressor = RandomForestRegressor(n_estimators = 200, min_samples_leaf=50, random_state = 0) \n",
    "  \n",
    "# fit the regressor with x and y data \n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "tr_pred = regressor.predict(X_train) \n",
    "te_pred = regressor.predict(X_test) \n",
    "\n",
    "print(\"RMSE for training set:\", sqrt(mean_squared_error(y_train, tr_pred)))\n",
    "print(\"RMSE for test set:\", sqrt(mean_squared_error(y_test, te_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stick with our parameters of 200 and 50. Next, let's adjust for the max_features parameter (the maximum number of features random forest considers to split a node). The default max_features is 'auto' so we will try 'sqrt' and 'log2' to see if test RMSE improves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def RF_Fit(X_train,y_train,X_test,y_test,num_estimators,min_samples_leafs,max_feat):\n",
    "    \n",
    "    #scaling the data\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \n",
    "     # create regressor object \n",
    "    regressor = RandomForestRegressor(n_estimators = num_estimators, min_samples_leaf=min_samples_leafs, max_features = max_feat, random_state = 0) \n",
    "  \n",
    "    # fit the regressor with x and y data \n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    tr_pred = regressor.predict(X_train) \n",
    "    te_pred = regressor.predict(X_test) \n",
    "    \n",
    "    print(\"RMSE for training set:\", sqrt(mean_squared_error(y_train, tr_pred)))\n",
    "    print(\"RMSE for test set:\", sqrt(mean_squared_error(y_test, te_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 15.175714465966099\n",
      "RMSE for test set: 15.141332288735486\n"
     ]
    }
   ],
   "source": [
    "RF_Fit(X_train,y_train,X_test,y_test,200,50,'sqrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 15.175714465966099\n",
      "RMSE for test set: 15.141332288735486\n"
     ]
    }
   ],
   "source": [
    "RF_Fit(X_train,y_train,X_test,y_test,200,50,'log2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default for max_features is 'auto'. If we recall from above, this achieved a test RMSE of 15.14 and train RMSE of 14.57 All of these models perform equally well. There is a small difference in the error for test and training set - indicating our model has found the sweet spot in the bias variance trade-off. Log2 and sqrt both diminish the variance of our model. Let's just stick with log2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating weekends and weekdays to see if it improves model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday = [0,1,2,3,4]\n",
    "weekday_tr = tr[tr['Weekday'].isin(weekday)]\n",
    "weekday_te = te[te['Weekday'].isin(weekday)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting X and Y variables\n",
    "y_train = weekday_tr['tv delta']\n",
    "X_train = weekday_tr[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]\n",
    "\n",
    "y_test = weekday_te['tv delta']\n",
    "X_test = weekday_te[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 16.27732440729296\n",
      "RMSE for test set: 15.769662053532306\n"
     ]
    }
   ],
   "source": [
    "RF_Fit(X_train,y_train,X_test,y_test,200,50,'log2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performed worse on weekday data alone. Let's leave the weekdays and weekends together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost \n",
    "\n",
    "Can we improve our model's performance?\n",
    "\n",
    "\n",
    "Random Forest achieved:\n",
    "- RMSE for training set: 15.175714465966099\n",
    "- RMSE for test set: 15.141332288735486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting X and Y variables\n",
    "y_train = tr['tv delta']\n",
    "X_train = tr[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]\n",
    "\n",
    "y_test = te['tv delta']\n",
    "X_test = te[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:54:13] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:130.23007\teval-rmse:110.43436\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 50 rounds.\n",
      "[1]\ttrain-rmse:117.69630\teval-rmse:98.91894\n",
      "[2]\ttrain-rmse:106.45283\teval-rmse:89.29782\n",
      "[3]\ttrain-rmse:96.31318\teval-rmse:80.06074\n",
      "[4]\ttrain-rmse:87.21256\teval-rmse:71.87453\n",
      "[5]\ttrain-rmse:79.04758\teval-rmse:64.67923\n",
      "[6]\ttrain-rmse:71.70111\teval-rmse:58.65943\n",
      "[7]\ttrain-rmse:65.12905\teval-rmse:53.10919\n",
      "[8]\ttrain-rmse:59.23132\teval-rmse:48.04398\n",
      "[9]\ttrain-rmse:53.96944\teval-rmse:43.51214\n",
      "[10]\ttrain-rmse:49.25400\teval-rmse:39.76111\n",
      "[11]\ttrain-rmse:45.06496\teval-rmse:36.30575\n",
      "[12]\ttrain-rmse:41.34412\teval-rmse:33.10768\n",
      "[13]\ttrain-rmse:38.00057\teval-rmse:30.40519\n",
      "[14]\ttrain-rmse:35.05162\teval-rmse:28.04753\n",
      "[15]\ttrain-rmse:32.41964\teval-rmse:26.11243\n",
      "[16]\ttrain-rmse:30.12803\teval-rmse:24.40588\n",
      "[17]\ttrain-rmse:28.10042\teval-rmse:22.93897\n",
      "[18]\ttrain-rmse:26.32119\teval-rmse:21.65026\n",
      "[19]\ttrain-rmse:24.75831\teval-rmse:20.59261\n",
      "[20]\ttrain-rmse:23.40969\teval-rmse:19.70712\n",
      "[21]\ttrain-rmse:22.22993\teval-rmse:18.88865\n",
      "[22]\ttrain-rmse:21.22350\teval-rmse:18.24254\n",
      "[23]\ttrain-rmse:20.35623\teval-rmse:17.67285\n",
      "[24]\ttrain-rmse:19.60899\teval-rmse:17.19724\n",
      "[25]\ttrain-rmse:18.97411\teval-rmse:16.83997\n",
      "[26]\ttrain-rmse:18.42912\teval-rmse:16.54213\n",
      "[27]\ttrain-rmse:17.97607\teval-rmse:16.29118\n",
      "[28]\ttrain-rmse:17.58740\teval-rmse:16.07340\n",
      "[29]\ttrain-rmse:17.26326\teval-rmse:15.92249\n",
      "[30]\ttrain-rmse:16.98875\teval-rmse:15.80185\n",
      "[31]\ttrain-rmse:16.74954\teval-rmse:15.69224\n",
      "[32]\ttrain-rmse:16.55727\teval-rmse:15.61033\n",
      "[33]\ttrain-rmse:16.39313\teval-rmse:15.53840\n",
      "[34]\ttrain-rmse:16.25148\teval-rmse:15.47889\n",
      "[35]\ttrain-rmse:16.13466\teval-rmse:15.43747\n",
      "[36]\ttrain-rmse:16.03593\teval-rmse:15.39382\n",
      "[37]\ttrain-rmse:15.95107\teval-rmse:15.35830\n",
      "[38]\ttrain-rmse:15.87730\teval-rmse:15.33425\n",
      "[39]\ttrain-rmse:15.81939\teval-rmse:15.31759\n",
      "[40]\ttrain-rmse:15.76629\teval-rmse:15.29845\n",
      "[41]\ttrain-rmse:15.72153\teval-rmse:15.28384\n",
      "[42]\ttrain-rmse:15.68360\teval-rmse:15.27276\n",
      "[43]\ttrain-rmse:15.64834\teval-rmse:15.26681\n",
      "[44]\ttrain-rmse:15.61617\teval-rmse:15.25458\n",
      "[45]\ttrain-rmse:15.58712\teval-rmse:15.24869\n",
      "[46]\ttrain-rmse:15.56511\teval-rmse:15.24261\n",
      "[47]\ttrain-rmse:15.54455\teval-rmse:15.24176\n",
      "[48]\ttrain-rmse:15.52373\teval-rmse:15.23977\n",
      "[49]\ttrain-rmse:15.50405\teval-rmse:15.23466\n",
      "[50]\ttrain-rmse:15.48978\teval-rmse:15.23227\n",
      "[51]\ttrain-rmse:15.47545\teval-rmse:15.23494\n",
      "[52]\ttrain-rmse:15.46047\teval-rmse:15.23497\n",
      "[53]\ttrain-rmse:15.44903\teval-rmse:15.23504\n",
      "[54]\ttrain-rmse:15.43832\teval-rmse:15.22952\n",
      "[55]\ttrain-rmse:15.42885\teval-rmse:15.23055\n",
      "[56]\ttrain-rmse:15.42028\teval-rmse:15.23282\n",
      "[57]\ttrain-rmse:15.41078\teval-rmse:15.23376\n",
      "[58]\ttrain-rmse:15.40051\teval-rmse:15.23203\n",
      "[59]\ttrain-rmse:15.39355\teval-rmse:15.22824\n",
      "[60]\ttrain-rmse:15.38620\teval-rmse:15.22984\n",
      "[61]\ttrain-rmse:15.37891\teval-rmse:15.23098\n",
      "[62]\ttrain-rmse:15.37321\teval-rmse:15.23227\n",
      "[63]\ttrain-rmse:15.36730\teval-rmse:15.23088\n",
      "[64]\ttrain-rmse:15.35569\teval-rmse:15.23044\n",
      "[65]\ttrain-rmse:15.35033\teval-rmse:15.23134\n",
      "[66]\ttrain-rmse:15.33850\teval-rmse:15.23283\n",
      "[67]\ttrain-rmse:15.32963\teval-rmse:15.23317\n",
      "[68]\ttrain-rmse:15.32442\teval-rmse:15.22924\n",
      "[69]\ttrain-rmse:15.31904\teval-rmse:15.23267\n",
      "[70]\ttrain-rmse:15.31345\teval-rmse:15.23303\n",
      "[71]\ttrain-rmse:15.30899\teval-rmse:15.23038\n",
      "[72]\ttrain-rmse:15.30110\teval-rmse:15.22717\n",
      "[73]\ttrain-rmse:15.29441\teval-rmse:15.22621\n",
      "[74]\ttrain-rmse:15.29106\teval-rmse:15.22677\n",
      "[75]\ttrain-rmse:15.28707\teval-rmse:15.22782\n",
      "[76]\ttrain-rmse:15.27621\teval-rmse:15.22801\n",
      "[77]\ttrain-rmse:15.27128\teval-rmse:15.22732\n",
      "[78]\ttrain-rmse:15.26746\teval-rmse:15.22363\n",
      "[79]\ttrain-rmse:15.25875\teval-rmse:15.22405\n",
      "[80]\ttrain-rmse:15.25502\teval-rmse:15.22659\n",
      "[81]\ttrain-rmse:15.25211\teval-rmse:15.22557\n",
      "[82]\ttrain-rmse:15.24886\teval-rmse:15.22680\n",
      "[83]\ttrain-rmse:15.24377\teval-rmse:15.22754\n",
      "[84]\ttrain-rmse:15.23659\teval-rmse:15.22763\n",
      "[85]\ttrain-rmse:15.22950\teval-rmse:15.22911\n",
      "[86]\ttrain-rmse:15.22336\teval-rmse:15.22949\n",
      "[87]\ttrain-rmse:15.21556\teval-rmse:15.22850\n",
      "[88]\ttrain-rmse:15.21342\teval-rmse:15.22871\n",
      "[89]\ttrain-rmse:15.21182\teval-rmse:15.23015\n",
      "[90]\ttrain-rmse:15.20811\teval-rmse:15.22689\n",
      "[91]\ttrain-rmse:15.20500\teval-rmse:15.22712\n",
      "[92]\ttrain-rmse:15.19939\teval-rmse:15.22873\n",
      "[93]\ttrain-rmse:15.19314\teval-rmse:15.22849\n",
      "[94]\ttrain-rmse:15.18993\teval-rmse:15.22949\n",
      "[95]\ttrain-rmse:15.18542\teval-rmse:15.22941\n",
      "[96]\ttrain-rmse:15.18054\teval-rmse:15.23029\n",
      "[97]\ttrain-rmse:15.17541\teval-rmse:15.23101\n",
      "[98]\ttrain-rmse:15.17410\teval-rmse:15.23171\n",
      "[99]\ttrain-rmse:15.17211\teval-rmse:15.23096\n",
      "[100]\ttrain-rmse:15.16753\teval-rmse:15.23129\n",
      "[101]\ttrain-rmse:15.16634\teval-rmse:15.23200\n",
      "[102]\ttrain-rmse:15.16242\teval-rmse:15.23251\n",
      "[103]\ttrain-rmse:15.15866\teval-rmse:15.23334\n",
      "[104]\ttrain-rmse:15.15469\teval-rmse:15.23166\n",
      "[105]\ttrain-rmse:15.15102\teval-rmse:15.23202\n",
      "[106]\ttrain-rmse:15.14922\teval-rmse:15.23336\n",
      "[107]\ttrain-rmse:15.14685\teval-rmse:15.23340\n",
      "[108]\ttrain-rmse:15.14558\teval-rmse:15.23270\n",
      "[109]\ttrain-rmse:15.14460\teval-rmse:15.23369\n",
      "[110]\ttrain-rmse:15.14257\teval-rmse:15.23361\n",
      "[111]\ttrain-rmse:15.13904\teval-rmse:15.23379\n",
      "[112]\ttrain-rmse:15.13570\teval-rmse:15.23648\n",
      "[113]\ttrain-rmse:15.13234\teval-rmse:15.23805\n",
      "[114]\ttrain-rmse:15.12936\teval-rmse:15.23579\n",
      "[115]\ttrain-rmse:15.12737\teval-rmse:15.23733\n",
      "[116]\ttrain-rmse:15.12584\teval-rmse:15.23811\n",
      "[117]\ttrain-rmse:15.12389\teval-rmse:15.23817\n",
      "[118]\ttrain-rmse:15.11969\teval-rmse:15.23466\n",
      "[119]\ttrain-rmse:15.11594\teval-rmse:15.23475\n",
      "[120]\ttrain-rmse:15.11303\teval-rmse:15.23511\n",
      "[121]\ttrain-rmse:15.11139\teval-rmse:15.23539\n",
      "[122]\ttrain-rmse:15.10972\teval-rmse:15.23658\n",
      "[123]\ttrain-rmse:15.10711\teval-rmse:15.23756\n",
      "[124]\ttrain-rmse:15.10477\teval-rmse:15.23885\n",
      "[125]\ttrain-rmse:15.10173\teval-rmse:15.24000\n",
      "[126]\ttrain-rmse:15.10096\teval-rmse:15.24028\n",
      "[127]\ttrain-rmse:15.09958\teval-rmse:15.24044\n",
      "[128]\ttrain-rmse:15.09676\teval-rmse:15.24214\n",
      "Stopping. Best iteration:\n",
      "[78]\ttrain-rmse:15.26746\teval-rmse:15.22363\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dtest = xgb.DMatrix(X_test, y_test)\n",
    "# model training\n",
    "params = {'seed': 33,\n",
    "          'objective': 'reg:squarederror',\n",
    "          'silent': 0,\n",
    "          'nthread': 1,\n",
    "          'max_depth': 2,\n",
    "          'learning_rate': 0.1}\n",
    "num_round = 5000\n",
    "evallist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "bst = xgb.train(params, dtrain, num_round, evallist, early_stopping_rounds=50)\n",
    "tr['pred'] = bst.predict(xgb.DMatrix(X_train))\n",
    "te['pred'] = bst.predict(xgb.DMatrix(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model actually performed better than XGBoost. \n",
    "XGBoost had the following metrics:\n",
    "- train-rmse:15.26746\t\n",
    "- test-rmse:15.22363"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size=64, beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "             learning_rate_init=0.005, max_fun=15000, max_iter=5000,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "y_train = tr['tv delta']\n",
    "X_train = tr[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]\n",
    "\n",
    "y_test = te['tv delta']\n",
    "X_test = te[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]\n",
    "\n",
    "#scaling the data\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "mlp = MLPRegressor(solver = 'adam',max_iter = 5000, batch_size = 64, learning_rate_init = 0.005 )\n",
    "\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "#add in dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 15.151045905727592\n",
      "RMSE for test set: 15.038969083530885\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "tr_pred = mlp.predict(X_train) \n",
    "te_pred = mlp.predict(X_test) \n",
    "    \n",
    "print(\"RMSE for training set:\", sqrt(mean_squared_error(y_train, tr_pred)))\n",
    "print(\"RMSE for test set:\", sqrt(mean_squared_error(y_test, te_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best random forest:\n",
    "- RMSE for training set: 15.1757\n",
    "- RMSE for test set: 15.1413\n",
    "- Difference = 0.0344\n",
    "\n",
    "XGBoost:\n",
    "- train-rmse:15.26745\n",
    "- test-rmse:15.2236\n",
    "- Difference = 0.0438\n",
    "\n",
    "NN before hypertuning parameters\n",
    "- RMSE for training set: 15.1510\n",
    "- RMSE for test set: 15.0389\n",
    "- Difference = 0.1121\n",
    "\n",
    "NN after hypertuning round 1: \n",
    "- RMSE for training set: 14.739\n",
    "- RMSE for test set: 15.169\n",
    "- Difference: 0.57\n",
    "\n",
    "NN after hypertuning round 2:\n",
    "- RMSE for training set: 15.2652\n",
    "- RMSE for test set: 15.2569\n",
    "- Difference: 0.0083"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'activation': 'tanh', 'batch_size': 20, 'hidden_layer_sizes': (50,), 'max_iter': 5000, 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "param_grid = [\n",
    "        {\n",
    "            'activation' : ['tanh', 'relu'],\n",
    "            'solver' : ['adam'],\n",
    "            'hidden_layer_sizes': [(5,),(15,),(50,)],\n",
    "            'batch_size': [20,60,100],\n",
    "            'max_iter' : [5000]\n",
    "        }\n",
    "       ]\n",
    "\n",
    "clf = GridSearchCV(MLPRegressor(), param_grid, cv=3,\n",
    "                           scoring='neg_root_mean_squared_error')\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing these parameters and looking at errors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 14.73862306420363\n",
      "RMSE for test set: 15.169115844584265\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(activation= 'tanh', solver = 'adam',max_iter = 5000, batch_size = 20,hidden_layer_sizes = (50,))\n",
    "\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "tr_pred = mlp.predict(X_train) \n",
    "te_pred = mlp.predict(X_test) \n",
    "    \n",
    "print(\"RMSE for training set:\", sqrt(mean_squared_error(y_train, tr_pred)))\n",
    "print(\"RMSE for test set:\", sqrt(mean_squared_error(y_test, te_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Narrowing in on the parameters by fine tuning again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'activation': 'tanh', 'batch_size': 20, 'hidden_layer_sizes': (15,), 'max_iter': 5000, 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "param_grid = [\n",
    "        {\n",
    "            'activation' : ['tanh'],\n",
    "            'solver' : ['adam'],\n",
    "            'hidden_layer_sizes': [(5,),(15,),(25,)],\n",
    "            'batch_size': [10,20,30],\n",
    "            'max_iter' : [5000]\n",
    "        }\n",
    "       ]\n",
    "\n",
    "clf = GridSearchCV(MLPRegressor(), param_grid, cv=3,\n",
    "                           scoring='neg_root_mean_squared_error')\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 15.265161072040847\n",
      "RMSE for test set: 15.256953660570046\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(activation= 'tanh', solver = 'adam',max_iter = 5000, batch_size = 20,hidden_layer_sizes = (15,))\n",
    "\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "tr_pred = mlp.predict(X_train) \n",
    "te_pred = mlp.predict(X_test) \n",
    "    \n",
    "print(\"RMSE for training set:\", sqrt(mean_squared_error(y_train, tr_pred)))\n",
    "print(\"RMSE for test set:\", sqrt(mean_squared_error(y_test, te_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary statistics of our target feature to understand context of RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tr.append(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    49001.000000\n",
       "mean       129.251563\n",
       "std         48.926223\n",
       "min          0.000000\n",
       "25%        100.000000\n",
       "50%        123.500000\n",
       "75%        151.000000\n",
       "max        425.000000\n",
       "Name: tv delta, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tv delta'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 15.175714465966099\n",
      "RMSE for test set: 15.141332288735486\n",
      "---------------------------------------------\n",
      "Variable: prev value           Importance: 0.47\n",
      "Variable: twice prev value     Importance: 0.3\n",
      "Variable: day shift            Importance: 0.19\n",
      "Variable: Temperature          Importance: 0.03\n",
      "Variable: holiday              Importance: 0.0\n",
      "Variable: month shift          Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### The best model: \n",
    "y_train = tr['tv delta']\n",
    "X_train = tr[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]\n",
    "\n",
    "y_test = te['tv delta']\n",
    "X_test = te[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]\n",
    "\n",
    "#scaling the data\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "    \n",
    "# create regressor object \n",
    "regressor = RandomForestRegressor(n_estimators = 200, min_samples_leaf=50, max_features = 'log2', random_state = 0) \n",
    "  \n",
    "# fit the regressor with x and y data \n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "tr_pred = regressor.predict(X_train) \n",
    "te_pred = regressor.predict(X_test) \n",
    "    \n",
    "print(\"RMSE for training set:\", sqrt(mean_squared_error(y_train, tr_pred)))\n",
    "print(\"RMSE for test set:\", sqrt(mean_squared_error(y_test, te_pred)))\n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "feature_list = ['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']\n",
    "# Get numerical feature importances\n",
    "importances = list(regressor.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for training set: 0.9081895390114595\n",
      "R2 for test set: 0.8783616677282491\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(\"R2 for training set:\", r2_score(y_train, tr_pred))\n",
    "print(\"R2 for test set:\", r2_score(y_test, te_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature contribution graphic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 468x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots() \n",
    "width = 0.4 # the width of the bars \n",
    "ind = np.arange(len(importances)) # the x locations for the groups\n",
    "ax.barh(ind, importances, width, color='skyblue')\n",
    "ax.set_yticks(ind+width/10)\n",
    "ax.set_yticklabels(feature_list, minor=False)\n",
    "plt.title('Feature importance in RandomForest Regressor')\n",
    "plt.xlabel('Relative importance')\n",
    "plt.ylabel('Feature') \n",
    "plt.figure(figsize=(5,5))\n",
    "fig.set_size_inches(6.5, 4.5, forward=True)\n",
    "plt.savefig('feature_imp.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['pred'] = tr_pred\n",
    "te['pred'] = te_pred\n",
    "\n",
    "te.to_csv('test_predictions.csv')\n",
    "tr.to_csv('train_predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
