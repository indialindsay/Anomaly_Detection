{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('train.csv')\n",
    "te = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "As we are using time series data, our variables are correlated. We cannot use any linear predictors. \n",
    "\n",
    "Adv:\n",
    "1. The random forest algorithm is not biased, since, there are multiple trees and each tree is trained on a subset of data. Basically, the random forest algorithm relies on the power of \"the crowd\"; therefore the overall biasedness of the algorithm is reduced.\n",
    "2. This algorithm is very stable. Even if a new data point is introduced in the dataset the overall algorithm is not affected much since new data may impact one tree, but it is very hard for it to impact all the trees.\n",
    "3. The random forest algorithm works well when you have both categorical and numerical features.\n",
    "4. The random forest algorithm also works well when data has missing values or it has not been scaled well (although we have performed feature scaling in this article just for the purpose of demonstration).\n",
    "\n",
    "Disadv:\n",
    "1. A major disadvantage of random forests lies in their complexity. They required much more computational resources, owing to the large number of decision trees joined together.\n",
    "2. Due to their complexity, they require much more time to train than other comparable algorithms.\n",
    "\n",
    "source: https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/#:~:text=%20The%20following%20are%20the%20basic%20steps%20involved,value%20for%20Y%20%28output%29.%20The%20final...%20More%20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from math import sqrt\n",
    "\n",
    "#fourier transform time.. keep low frequency vs high frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Timestamp', 'Values', 'Temperature', 'holiday',\n",
       "       'Weekday', 'Hour', 'Month', 'Day', 'Time Delta', 'value delta',\n",
       "       'tv delta', 'prev value', 'twice prev value', 'day shift',\n",
       "       'month shift', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial random forest fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting X and Y variables\n",
    "y_train = tr['tv delta']\n",
    "X_train = tr[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]\n",
    "\n",
    "y_test = te['tv delta']\n",
    "X_test = te[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 5.716350014724802\n",
      "RMSE for test set: 15.887416299573475\n",
      "Mean Absolute Error for train: 3.949370992318209\n",
      "Mean Absolute Error for test: 11.002088145230802\n",
      "Variable: prev value           Importance: 0.9\n",
      "Variable: Temperature          Importance: 0.03\n",
      "Variable: day shift            Importance: 0.03\n",
      "Variable: twice prev value     Importance: 0.02\n",
      "Variable: month shift          Importance: 0.02\n",
      "Variable: holiday              Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#scaling the data\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# create regressor object \n",
    "regressor = RandomForestRegressor(n_estimators = 100, random_state = 0) \n",
    "  \n",
    "# fit the regressor with x and y data \n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "tr_pred = regressor.predict(X_train) \n",
    "te_pred = regressor.predict(X_test) \n",
    "\n",
    "print(\"RMSE for training set:\", sqrt(mean_squared_error(y_train, tr_pred)))\n",
    "print(\"RMSE for test set:\", sqrt(mean_squared_error(y_test, te_pred)))\n",
    "\n",
    "print('Mean Absolute Error for train:', metrics.mean_absolute_error(y_train, tr_pred))\n",
    "print('Mean Absolute Error for test:', metrics.mean_absolute_error(y_test, te_pred))\n",
    "\n",
    "feature_list = ['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']\n",
    "# Get numerical feature importances\n",
    "importances = list(regressor.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are clearly overfitting. Let's finetune some parameters\n",
    "\n",
    "- n_estimators: number of trees built before taking avg of predictions. In general, a higher number of trees increases the performance and makes the predictions more stable, but it also slows down the computation\n",
    "\n",
    "- min_sample_leaf: The minimum number of samples required to be at a leaf node. A smaller leaf makes the model more prone to capturing noise in train data. We are overfitting so let's try increasing this.\n",
    "\n",
    "source: https://builtin.com/data-science/random-forest-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning(X_train,y_train,X_test,y_test, estimators, leaf):\n",
    "    ''' This function takes a list of estimator and leaf parameters and returns \n",
    "    the parameters that produced the lowest error'''\n",
    "    \n",
    "    arr = np.zeros((len(estimators),len(leaf))) #empty array to store scores\n",
    "    \n",
    "    #scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    for i in range(len(estimators)):\n",
    "        for j in range(len(leaf)):\n",
    "            est = estimators[i]\n",
    "            l = leaf[j]\n",
    "           \n",
    "             # create regressor object \n",
    "            regressor = RandomForestRegressor(n_estimators = est, min_samples_leaf=l, random_state = 0) \n",
    "  \n",
    "            # fit the regressor with x and y data \n",
    "            regressor.fit(X_train, y_train)\n",
    "\n",
    "            tr_pred = regressor.predict(X_train) \n",
    "            te_pred = regressor.predict(X_test) \n",
    "\n",
    "            test_score = sqrt(mean_squared_error(y_test, te_pred))\n",
    "            arr[i][j] = test_score\n",
    "    \n",
    "   \n",
    "    max_ = np.amin(arr) \n",
    "    inputs = np.where(arr == max_)\n",
    "    print(\"Lowest test RMSE score:\",max_)\n",
    "    best_est = estimators[inputs[0][0]]\n",
    "    best_leaf = leaf[inputs[1][0]]\n",
    "    print(\"Best # estimators:\",best_est)\n",
    "    print(\"Best # min samples leaf:\",best_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest test RMSE score: 15.140363398825533\n",
      "Best # estimators: 300\n",
      "Best # min samples leaf: 50\n"
     ]
    }
   ],
   "source": [
    "est = [10,100,200,300]\n",
    "leaf = [15,30,50,70]\n",
    "max_feats = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "arr = tuning(X_train,y_train,X_test,y_test,est,leaf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test RMSE dropped from the previous value of 15.88 to 15.14\n",
    "\n",
    "Let's see if increasing both the number of estimators and min samples leaf value will help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest test RMSE score: 15.140363398825533\n",
      "Best # estimators: 300\n",
      "Best # min samples leaf: 50\n"
     ]
    }
   ],
   "source": [
    "est = [300,500,700]\n",
    "leaf = [50,60,70]\n",
    "\n",
    "arr = tuning(X_train,y_train,X_test,y_test,est,leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300 and 50 is our best bet. Let's see whether 200 will perform as well as 300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 14.570057072199504\n",
      "RMSE for test set: 15.143103582113675\n"
     ]
    }
   ],
   "source": [
    "regressor = RandomForestRegressor(n_estimators = 200, min_samples_leaf=50, random_state = 0) \n",
    "  \n",
    "# fit the regressor with x and y data \n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "tr_pred = regressor.predict(X_train) \n",
    "te_pred = regressor.predict(X_test) \n",
    "\n",
    "print(\"RMSE for training set:\", sqrt(mean_squared_error(y_train, tr_pred)))\n",
    "print(\"RMSE for test set:\", sqrt(mean_squared_error(y_test, te_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stick with our parameters of 200 and 50. Next, let's adjust for the max_features parameter (the maximum number of features random forest considers to split a node). The default max_features is 'auto' so we will try 'sqrt' and 'log2' to see if test RMSE improves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def RF_Fit(X_train,y_train,X_test,y_test,num_estimators,min_samples_leafs,max_feat):\n",
    "    \n",
    "    #scaling the data\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \n",
    "     # create regressor object \n",
    "    regressor = RandomForestRegressor(n_estimators = num_estimators, min_samples_leaf=min_samples_leafs, max_features = max_feat, random_state = 0) \n",
    "  \n",
    "    # fit the regressor with x and y data \n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    tr_pred = regressor.predict(X_train) \n",
    "    te_pred = regressor.predict(X_test) \n",
    "    \n",
    "    print(\"RMSE for training set:\", sqrt(mean_squared_error(y_train, tr_pred)))\n",
    "    print(\"RMSE for test set:\", sqrt(mean_squared_error(y_test, te_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 15.175714465966099\n",
      "RMSE for test set: 15.141332288735486\n"
     ]
    }
   ],
   "source": [
    "RF_Fit(X_train,y_train,X_test,y_test,200,50,'sqrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 15.175714465966099\n",
      "RMSE for test set: 15.141332288735486\n"
     ]
    }
   ],
   "source": [
    "RF_Fit(X_train,y_train,X_test,y_test,200,50,'log2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default for max_features is 'auto'. If we recall from above, this achieved a test RMSE of 15.14 and train RMSE of 14.57 All of these models perform equally well. There is a small difference in the error for test and training set - indicating our model has found the sweet spot in the bias variance trade-off. Log2 and sqrt both diminish the variance of our model. Let's just stick with log2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating weekends and weekdays to see if it improves model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday = [0,1,2,3,4]\n",
    "weekday_tr = tr[tr['Weekday'].isin(weekday)]\n",
    "weekday_te = te[te['Weekday'].isin(weekday)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting X and Y variables\n",
    "y_train = weekday_tr['tv delta']\n",
    "X_train = weekday_tr[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]\n",
    "\n",
    "y_test = weekday_te['tv delta']\n",
    "X_test = weekday_te[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 16.27732440729296\n",
      "RMSE for test set: 15.769662053532306\n"
     ]
    }
   ],
   "source": [
    "RF_Fit(X_train,y_train,X_test,y_test,200,50,'log2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performed worse on weekday data alone. Let's leave the weekdays and weekends together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost \n",
    "\n",
    "Can we improve our model's performance?\n",
    "\n",
    "\n",
    "Random Forest achieved:\n",
    "- RMSE for training set: 15.175714465966099\n",
    "- RMSE for test set: 15.141332288735486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting X and Y variables\n",
    "y_train = tr['tv delta']\n",
    "X_train = tr[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]\n",
    "\n",
    "y_test = te['tv delta']\n",
    "X_test = te[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:54:13] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:130.23007\teval-rmse:110.43436\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 50 rounds.\n",
      "[1]\ttrain-rmse:117.69630\teval-rmse:98.91894\n",
      "[2]\ttrain-rmse:106.45283\teval-rmse:89.29782\n",
      "[3]\ttrain-rmse:96.31318\teval-rmse:80.06074\n",
      "[4]\ttrain-rmse:87.21256\teval-rmse:71.87453\n",
      "[5]\ttrain-rmse:79.04758\teval-rmse:64.67923\n",
      "[6]\ttrain-rmse:71.70111\teval-rmse:58.65943\n",
      "[7]\ttrain-rmse:65.12905\teval-rmse:53.10919\n",
      "[8]\ttrain-rmse:59.23132\teval-rmse:48.04398\n",
      "[9]\ttrain-rmse:53.96944\teval-rmse:43.51214\n",
      "[10]\ttrain-rmse:49.25400\teval-rmse:39.76111\n",
      "[11]\ttrain-rmse:45.06496\teval-rmse:36.30575\n",
      "[12]\ttrain-rmse:41.34412\teval-rmse:33.10768\n",
      "[13]\ttrain-rmse:38.00057\teval-rmse:30.40519\n",
      "[14]\ttrain-rmse:35.05162\teval-rmse:28.04753\n",
      "[15]\ttrain-rmse:32.41964\teval-rmse:26.11243\n",
      "[16]\ttrain-rmse:30.12803\teval-rmse:24.40588\n",
      "[17]\ttrain-rmse:28.10042\teval-rmse:22.93897\n",
      "[18]\ttrain-rmse:26.32119\teval-rmse:21.65026\n",
      "[19]\ttrain-rmse:24.75831\teval-rmse:20.59261\n",
      "[20]\ttrain-rmse:23.40969\teval-rmse:19.70712\n",
      "[21]\ttrain-rmse:22.22993\teval-rmse:18.88865\n",
      "[22]\ttrain-rmse:21.22350\teval-rmse:18.24254\n",
      "[23]\ttrain-rmse:20.35623\teval-rmse:17.67285\n",
      "[24]\ttrain-rmse:19.60899\teval-rmse:17.19724\n",
      "[25]\ttrain-rmse:18.97411\teval-rmse:16.83997\n",
      "[26]\ttrain-rmse:18.42912\teval-rmse:16.54213\n",
      "[27]\ttrain-rmse:17.97607\teval-rmse:16.29118\n",
      "[28]\ttrain-rmse:17.58740\teval-rmse:16.07340\n",
      "[29]\ttrain-rmse:17.26326\teval-rmse:15.92249\n",
      "[30]\ttrain-rmse:16.98875\teval-rmse:15.80185\n",
      "[31]\ttrain-rmse:16.74954\teval-rmse:15.69224\n",
      "[32]\ttrain-rmse:16.55727\teval-rmse:15.61033\n",
      "[33]\ttrain-rmse:16.39313\teval-rmse:15.53840\n",
      "[34]\ttrain-rmse:16.25148\teval-rmse:15.47889\n",
      "[35]\ttrain-rmse:16.13466\teval-rmse:15.43747\n",
      "[36]\ttrain-rmse:16.03593\teval-rmse:15.39382\n",
      "[37]\ttrain-rmse:15.95107\teval-rmse:15.35830\n",
      "[38]\ttrain-rmse:15.87730\teval-rmse:15.33425\n",
      "[39]\ttrain-rmse:15.81939\teval-rmse:15.31759\n",
      "[40]\ttrain-rmse:15.76629\teval-rmse:15.29845\n",
      "[41]\ttrain-rmse:15.72153\teval-rmse:15.28384\n",
      "[42]\ttrain-rmse:15.68360\teval-rmse:15.27276\n",
      "[43]\ttrain-rmse:15.64834\teval-rmse:15.26681\n",
      "[44]\ttrain-rmse:15.61617\teval-rmse:15.25458\n",
      "[45]\ttrain-rmse:15.58712\teval-rmse:15.24869\n",
      "[46]\ttrain-rmse:15.56511\teval-rmse:15.24261\n",
      "[47]\ttrain-rmse:15.54455\teval-rmse:15.24176\n",
      "[48]\ttrain-rmse:15.52373\teval-rmse:15.23977\n",
      "[49]\ttrain-rmse:15.50405\teval-rmse:15.23466\n",
      "[50]\ttrain-rmse:15.48978\teval-rmse:15.23227\n",
      "[51]\ttrain-rmse:15.47545\teval-rmse:15.23494\n",
      "[52]\ttrain-rmse:15.46047\teval-rmse:15.23497\n",
      "[53]\ttrain-rmse:15.44903\teval-rmse:15.23504\n",
      "[54]\ttrain-rmse:15.43832\teval-rmse:15.22952\n",
      "[55]\ttrain-rmse:15.42885\teval-rmse:15.23055\n",
      "[56]\ttrain-rmse:15.42028\teval-rmse:15.23282\n",
      "[57]\ttrain-rmse:15.41078\teval-rmse:15.23376\n",
      "[58]\ttrain-rmse:15.40051\teval-rmse:15.23203\n",
      "[59]\ttrain-rmse:15.39355\teval-rmse:15.22824\n",
      "[60]\ttrain-rmse:15.38620\teval-rmse:15.22984\n",
      "[61]\ttrain-rmse:15.37891\teval-rmse:15.23098\n",
      "[62]\ttrain-rmse:15.37321\teval-rmse:15.23227\n",
      "[63]\ttrain-rmse:15.36730\teval-rmse:15.23088\n",
      "[64]\ttrain-rmse:15.35569\teval-rmse:15.23044\n",
      "[65]\ttrain-rmse:15.35033\teval-rmse:15.23134\n",
      "[66]\ttrain-rmse:15.33850\teval-rmse:15.23283\n",
      "[67]\ttrain-rmse:15.32963\teval-rmse:15.23317\n",
      "[68]\ttrain-rmse:15.32442\teval-rmse:15.22924\n",
      "[69]\ttrain-rmse:15.31904\teval-rmse:15.23267\n",
      "[70]\ttrain-rmse:15.31345\teval-rmse:15.23303\n",
      "[71]\ttrain-rmse:15.30899\teval-rmse:15.23038\n",
      "[72]\ttrain-rmse:15.30110\teval-rmse:15.22717\n",
      "[73]\ttrain-rmse:15.29441\teval-rmse:15.22621\n",
      "[74]\ttrain-rmse:15.29106\teval-rmse:15.22677\n",
      "[75]\ttrain-rmse:15.28707\teval-rmse:15.22782\n",
      "[76]\ttrain-rmse:15.27621\teval-rmse:15.22801\n",
      "[77]\ttrain-rmse:15.27128\teval-rmse:15.22732\n",
      "[78]\ttrain-rmse:15.26746\teval-rmse:15.22363\n",
      "[79]\ttrain-rmse:15.25875\teval-rmse:15.22405\n",
      "[80]\ttrain-rmse:15.25502\teval-rmse:15.22659\n",
      "[81]\ttrain-rmse:15.25211\teval-rmse:15.22557\n",
      "[82]\ttrain-rmse:15.24886\teval-rmse:15.22680\n",
      "[83]\ttrain-rmse:15.24377\teval-rmse:15.22754\n",
      "[84]\ttrain-rmse:15.23659\teval-rmse:15.22763\n",
      "[85]\ttrain-rmse:15.22950\teval-rmse:15.22911\n",
      "[86]\ttrain-rmse:15.22336\teval-rmse:15.22949\n",
      "[87]\ttrain-rmse:15.21556\teval-rmse:15.22850\n",
      "[88]\ttrain-rmse:15.21342\teval-rmse:15.22871\n",
      "[89]\ttrain-rmse:15.21182\teval-rmse:15.23015\n",
      "[90]\ttrain-rmse:15.20811\teval-rmse:15.22689\n",
      "[91]\ttrain-rmse:15.20500\teval-rmse:15.22712\n",
      "[92]\ttrain-rmse:15.19939\teval-rmse:15.22873\n",
      "[93]\ttrain-rmse:15.19314\teval-rmse:15.22849\n",
      "[94]\ttrain-rmse:15.18993\teval-rmse:15.22949\n",
      "[95]\ttrain-rmse:15.18542\teval-rmse:15.22941\n",
      "[96]\ttrain-rmse:15.18054\teval-rmse:15.23029\n",
      "[97]\ttrain-rmse:15.17541\teval-rmse:15.23101\n",
      "[98]\ttrain-rmse:15.17410\teval-rmse:15.23171\n",
      "[99]\ttrain-rmse:15.17211\teval-rmse:15.23096\n",
      "[100]\ttrain-rmse:15.16753\teval-rmse:15.23129\n",
      "[101]\ttrain-rmse:15.16634\teval-rmse:15.23200\n",
      "[102]\ttrain-rmse:15.16242\teval-rmse:15.23251\n",
      "[103]\ttrain-rmse:15.15866\teval-rmse:15.23334\n",
      "[104]\ttrain-rmse:15.15469\teval-rmse:15.23166\n",
      "[105]\ttrain-rmse:15.15102\teval-rmse:15.23202\n",
      "[106]\ttrain-rmse:15.14922\teval-rmse:15.23336\n",
      "[107]\ttrain-rmse:15.14685\teval-rmse:15.23340\n",
      "[108]\ttrain-rmse:15.14558\teval-rmse:15.23270\n",
      "[109]\ttrain-rmse:15.14460\teval-rmse:15.23369\n",
      "[110]\ttrain-rmse:15.14257\teval-rmse:15.23361\n",
      "[111]\ttrain-rmse:15.13904\teval-rmse:15.23379\n",
      "[112]\ttrain-rmse:15.13570\teval-rmse:15.23648\n",
      "[113]\ttrain-rmse:15.13234\teval-rmse:15.23805\n",
      "[114]\ttrain-rmse:15.12936\teval-rmse:15.23579\n",
      "[115]\ttrain-rmse:15.12737\teval-rmse:15.23733\n",
      "[116]\ttrain-rmse:15.12584\teval-rmse:15.23811\n",
      "[117]\ttrain-rmse:15.12389\teval-rmse:15.23817\n",
      "[118]\ttrain-rmse:15.11969\teval-rmse:15.23466\n",
      "[119]\ttrain-rmse:15.11594\teval-rmse:15.23475\n",
      "[120]\ttrain-rmse:15.11303\teval-rmse:15.23511\n",
      "[121]\ttrain-rmse:15.11139\teval-rmse:15.23539\n",
      "[122]\ttrain-rmse:15.10972\teval-rmse:15.23658\n",
      "[123]\ttrain-rmse:15.10711\teval-rmse:15.23756\n",
      "[124]\ttrain-rmse:15.10477\teval-rmse:15.23885\n",
      "[125]\ttrain-rmse:15.10173\teval-rmse:15.24000\n",
      "[126]\ttrain-rmse:15.10096\teval-rmse:15.24028\n",
      "[127]\ttrain-rmse:15.09958\teval-rmse:15.24044\n",
      "[128]\ttrain-rmse:15.09676\teval-rmse:15.24214\n",
      "Stopping. Best iteration:\n",
      "[78]\ttrain-rmse:15.26746\teval-rmse:15.22363\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dtest = xgb.DMatrix(X_test, y_test)\n",
    "# model training\n",
    "params = {'seed': 33,\n",
    "          'objective': 'reg:squarederror',\n",
    "          'silent': 0,\n",
    "          'nthread': 1,\n",
    "          'max_depth': 2,\n",
    "          'learning_rate': 0.1}\n",
    "num_round = 5000\n",
    "evallist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "bst = xgb.train(params, dtrain, num_round, evallist, early_stopping_rounds=50)\n",
    "tr['pred'] = bst.predict(xgb.DMatrix(X_train))\n",
    "te['pred'] = bst.predict(xgb.DMatrix(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model actually performed better than XGBoost. \n",
    "XGBoost had the following metrics:\n",
    "- train-rmse:15.26746\t\n",
    "- test-rmse:15.22363\n",
    "\n",
    "### The best model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 15.175714465966099\n",
      "RMSE for test set: 15.141332288735486\n"
     ]
    }
   ],
   "source": [
    "y_train = tr['tv delta']\n",
    "X_train = tr[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]\n",
    "\n",
    "y_test = te['tv delta']\n",
    "X_test = te[['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']]\n",
    "\n",
    "#scaling the data\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "    \n",
    "# create regressor object \n",
    "regressor = RandomForestRegressor(n_estimators = 200, min_samples_leaf=50, max_features = 'log2', random_state = 0) \n",
    "  \n",
    "# fit the regressor with x and y data \n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "tr_pred = regressor.predict(X_train) \n",
    "te_pred = regressor.predict(X_test) \n",
    "    \n",
    "print(\"RMSE for training set:\", sqrt(mean_squared_error(y_train, tr_pred)))\n",
    "print(\"RMSE for test set:\", sqrt(mean_squared_error(y_test, te_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: prev value           Importance: 0.9\n",
      "Variable: Temperature          Importance: 0.03\n",
      "Variable: day shift            Importance: 0.03\n",
      "Variable: twice prev value     Importance: 0.02\n",
      "Variable: month shift          Importance: 0.02\n",
      "Variable: holiday              Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "feature_list = ['Temperature', 'holiday', 'prev value','twice prev value','day shift','month shift']\n",
    "# Get numerical feature importances\n",
    "importances = list(regressor.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['pred'] = tr_pred\n",
    "te['pred'] = te_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "te.to_csv('test_predictions.csv')\n",
    "tr.to_csv('train_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size=64, beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "             learning_rate_init=0.005, max_fun=15000, max_iter=5000,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#scaling data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "mlp = MLPRegressor(solver = 'adam',max_iter = 5000, batch_size = 64, learning_rate_init = 0.005 )\n",
    "\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "#add in dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set: 15.381277809035472\n",
      "RMSE for test set: 14.994611090819559\n"
     ]
    }
   ],
   "source": [
    "tr_pred = mlp.predict(X_train) \n",
    "te_pred = mlp.predict(X_test) \n",
    "    \n",
    "print(\"RMSE for training set:\", sqrt(mean_squared_error(y_train, tr_pred)))\n",
    "print(\"RMSE for test set:\", sqrt(mean_squared_error(y_test, te_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\india\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\india\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\india\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\india\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\india\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "C:\\Users\\india\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "param_grid = [\n",
    "        {\n",
    "            'activation' : ['tanh', 'relu'],\n",
    "            'solver' : ['adam'],\n",
    "            'hidden_layer_sizes': [(5,),(15,),(50,),(100,)],\n",
    "            'batch_size': [20,200,600]\n",
    "        }\n",
    "       ]\n",
    "\n",
    "clf = GridSearchCV(MLPRegressor(), param_grid, cv=3,\n",
    "                           scoring='neg_root_mean_squared_error')\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
